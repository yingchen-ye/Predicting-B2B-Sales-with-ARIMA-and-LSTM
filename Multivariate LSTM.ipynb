{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from keras import callbacks, regularizers, optimizers\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.regularizers import L1L2\n",
    "from keras_tuner import RandomSearch, Objective\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN IT: define a function check_stationarity(series)\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def check_stationarity(series):\n",
    "\n",
    "    result = adfuller(series.values)\n",
    "\n",
    "    print('ADF Statistic: %f' % result[0])\n",
    "    print('p-value: %f' % result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "    if (result[1] <= 0.05) & (result[4]['5%'] > result[0]):\n",
    "        print(\"\\u001b[32mStationary\\u001b[0m\")\n",
    "    else:\n",
    "        print(\"\\x1b[31mNon-stationary\\x1b[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('./data/cleaned_raw_data.csv', index_col=None)\n",
    "# deal with negative data 100904, 101990\n",
    "raw_data = raw_data.drop(raw_data.index[[14597, 17480, 28433, 29224, 271980, 100904, 101990, 212036, 211017, 198306]], axis=0)  # drop a negative value that influence the sum\n",
    "weekly_data = raw_data.loc[:,['Week', 'ENT Area', 'Product Platform', 'Line Item Net New TCV']]\n",
    "weekly_sum = weekly_data.groupby(['Week', 'ENT Area', 'Product Platform']).agg({'Line Item Net New TCV': 'sum'}).reset_index()\n",
    "# weekly_sum.to_csv('./data/weekly_sum_categorical.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in weekly_sum.columns:\n",
    "    print(x ,':', len(weekly_sum[x].unique()))\n",
    "\n",
    "# the sum of HQ is 0, so not show here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding the Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sum = pd.read_csv('./data/weekly_sum_categorical.csv')\n",
    "ohe = OneHotEncoder(categories='auto')\n",
    "feature_arr = ohe.fit_transform(weekly_sum[['ENT Area','Product Platform']]).toarray()\n",
    "feature_labels = ohe.categories_\n",
    "feature_labels = ohe.get_feature_names_out()\n",
    "features = pd.DataFrame(feature_arr, columns=feature_labels)\n",
    "features['bookings'] = weekly_sum['Line Item Net New TCV']\n",
    "features.insert(0,'week', weekly_sum['Week'])\n",
    "# features.to_csv('./data/encoded_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot table by Product and Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sum = pd.read_csv('./data/weekly_sum_categorical.csv' )\n",
    "product_sum = pd.pivot_table(weekly_sum, values='Line Item Net New TCV', index='Week', columns='Product Platform', aggfunc=np.sum, fill_value=0, margins=True).iloc[:-1, :]\n",
    "# product_sum.to_csv('./data/product sum.csv')\n",
    "area_sum = pd.pivot_table(weekly_sum, values='Line Item Net New TCV', index='Week', columns='ENT Area', aggfunc=np.sum, fill_value=0, margins=True).iloc[:-1, :]\n",
    "# area_sum.to_csv('./data/area sum.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the data by classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_categorical_transition(df, col, row_n, col_n):\n",
    "    category = df[col].unique().tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(row_n, col_n, figsize=(12, 9))\n",
    "    ax = axes.ravel()\n",
    "    \n",
    "    for i, cate in enumerate(category):\n",
    "        x = df[df[col] == cate].groupby(\"Week\").mean().loc[:, [\"Line Item Net New TCV\"]]\n",
    "        x.plot(ax=ax[i])\n",
    "        ax[i].set_title(cate)\n",
    "        ax[i].set_xlabel(\"\")\n",
    "        \n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_categorical_transition(weekly_sum, 'ENT Area', 3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_categorical_transition(weekly_sum, 'Product Platform', 3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Seperately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data, targets, n_input):\n",
    "    generator = TimeseriesGenerator(data = data, targets= targets, length=n_input, batch_size=1)\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to report performance\n",
    "def lstm_report(y_true, y_pred):\n",
    "    # measures on validation set\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    return rmse, mse, mae, mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area as Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note!!! Two training/test sets have the same variable names. Plz run then from defining the datasets when training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset, transfer to supervised learning. The target variable is the total bookings for each week.\n",
    "area_sum = pd.read_csv('./data/area sum.csv', index_col=0, parse_dates=True).squeeze(\"columns\")\n",
    "\n",
    "test_size = 52\n",
    "data_train, data_test =  area_sum.iloc[ :-test_size,:], area_sum.iloc[-test_size:,:]\n",
    "\n",
    "# log transformation\n",
    "data_train, data_test = np.log(data_train+1), np.log(data_test+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transformation and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset for normalization\n",
    "train_X = data_train.iloc[:,:-1]\n",
    "train_y = np.array(data_train.iloc[:,-1]).reshape(-1,1)\n",
    "test_X = data_test.iloc[:,:-1]\n",
    "test_y = np.array(data_test.iloc[:,-1]).reshape(-1,1)\n",
    "\n",
    "# normalization\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "scaler_X.fit(train_X)\n",
    "scaled_train_X = scaler_X.transform(train_X)\n",
    "scaled_test_X = scaler_X.transform(test_X)\n",
    "\n",
    "scaler_y.fit(train_y)\n",
    "scaled_train_y = scaler_y.transform(train_y)\n",
    "scaled_test_y = scaler_y.transform(test_y)\n",
    "\n",
    "scaled_X = np.append(scaled_train_X, scaled_test_X, axis=0)\n",
    "\n",
    "print(scaled_train_X.shape, scaled_test_X.shape, scaled_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "def create_vanilla(n_input, n_features, neuron):\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(neuron, activation='relu', input_shape = (n_input, n_features)))\n",
    "    lstm_model.add(Dense(1))\n",
    "    lstm_model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError(), metrics=['mse'])\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# walking forward validation\n",
    "def predict_lstm(n_input, n_feature, model, data_test, data):\n",
    "    lstm_predictions_scaled = list()\n",
    "\n",
    "    for i in range(data_test.shape[0]): \n",
    "        batch = data[-52-n_input:-1, :][i:i+n_input, :]\n",
    "        current_batch = batch.reshape((1, n_input, n_feature))\n",
    "        lstm_pred = model.predict(current_batch, verbose=0)[0]\n",
    "        lstm_predictions_scaled.append(lstm_pred)\n",
    "    lstm_predictions = scaler_y.inverse_transform(lstm_predictions_scaled)\n",
    "    lstm_predictions = pd.Series(lstm_predictions.reshape(1,52)[0], index=data_test.index)\n",
    "    return lstm_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_area = create_vanilla(13, 9, 512)\n",
    "vanilla_area.fit(data_generator(scaled_train_X, scaled_train_y, 13),epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Loss History of Multivariate Vanilla LSTM')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(vanilla_area.history.history['loss'], label = \"loss\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection = pd.DataFrame(columns=['rmse', 'mse', 'mae', 'mape'])\n",
    "\n",
    "model_selection.loc['Multivariate Vanilla LSTM, Area'] = \\\n",
    "                    list(lstm_report(test_y, \n",
    "                         predict_lstm(13, 9, vanilla_area, data_test, scaled_X)))\n",
    "model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_true = pd.Series(test_y.reshape(-1,), index = data_test.index,)\n",
    "lstm_predictions = predict_lstm(13, 9, vanilla_area, data_test, scaled_X)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lstm_true, label = 'y_true')\n",
    "plt.plot(lstm_predictions, label = 'y_pred')\n",
    "plt.title('Multivariate Vanilla LSTM Predictions, Area')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked model\n",
    "def create_2_stacked(n_input, n_features, neuron1, neuron2):\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(neuron1, activation='relu', input_shape = (n_input, n_features), return_sequences=True))\n",
    "    lstm_model.add(LSTM(neuron2))\n",
    "    lstm_model.add(Dense(1))\n",
    "    lstm_model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError(), metrics=['mse'])\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_area = create_2_stacked(13, 9, 512, 512)\n",
    "stacked_area.fit(data_generator(scaled_train_X, scaled_train_y, 13),epochs=20)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Loss History of Multivariate Stacked LSTM')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(stacked_area.history.history['loss'], label = \"loss\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "lstm_true = pd.Series(test_y.reshape(-1,), index = data_test.index,)\n",
    "lstm_predictions = predict_lstm(13, 9, stacked_area, data_test, scaled_X)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lstm_true, label = 'y_true')\n",
    "plt.plot(lstm_predictions, label = 'y_pred')\n",
    "plt.title('Multivariate Stacked LSTM Predictions, Area')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection.loc['Multivariate Stacked LSTM, Area, Hidden Layer = 2'] = \\\n",
    "                    list(lstm_report(test_y, \n",
    "                         predict_lstm(13, 9, stacked_area, data_test, scaled_X)))\n",
    "model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM, Hidden layer = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked model\n",
    "def create_3_stacked(n_input, n_features, neuron1, neuron2, neuron3):\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(neuron1, activation='relu', input_shape = (n_input, n_features), return_sequences=True))\n",
    "    lstm_model.add(LSTM(neuron2, return_sequences=True))\n",
    "    lstm_model.add(LSTM(neuron3))\n",
    "    lstm_model.add(Dense(1))\n",
    "    lstm_model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError(), metrics=['mse'])\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_area2 = create_3_stacked(13, 9, 512, 512, 512)\n",
    "stacked_area2.fit(data_generator(scaled_train_X, scaled_train_y, 13),epochs=20)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Loss History of Multivariate Stacked LSTM')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(stacked_area2.history.history['loss'], label = \"loss\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "lstm_true = pd.Series(test_y.reshape(-1,), index = data_test.index,)\n",
    "lstm_predictions = predict_lstm(13, 9, stacked_area2, data_test, scaled_X)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lstm_true, label = 'y_true')\n",
    "plt.plot(lstm_predictions, label = 'y_pred')\n",
    "plt.title('Multivariate Stacked LSTM Predictions, Area')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection.loc['Multivariate Stacked LSTM, Area, Hidden Layer = 3'] = \\\n",
    "                    list(lstm_report(test_y, \n",
    "                         predict_lstm(13, 9, stacked_area2, data_test, scaled_X)))\n",
    "model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM, Hidden Layer = 2, with regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# might be overfitting. Let's add regularizers to stacked_area\n",
    "\n",
    "def create_2_stacked2(n_input, n_features, neuron1, neuron2, dropout_rate):\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(neuron1, activation='relu', input_shape = (n_input, n_features), return_sequences=True))\n",
    "    lstm_model.add(LSTM(neuron2, recurrent_dropout = dropout_rate, recurrent_regularizer=L1L2(l1=0.001, l2=0.001)))\n",
    "    lstm_model.add(Dense(1))\n",
    "    lstm_model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError(), metrics=['mse'])\n",
    "    return lstm_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_area3 = create_2_stacked2(13, 9, 512, 512, 512, 0.1)\n",
    "stacked_area3.fit(data_generator(scaled_train_X, scaled_train_y, 13),epochs=20)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Loss History of Multivariate Stacked LSTM')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(stacked_area3.history.history['loss'], label = \"loss\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "lstm_true = pd.Series(test_y.reshape(-1,), index = data_test.index,)\n",
    "lstm_predictions = predict_lstm(13, 9, stacked_area3, data_test, scaled_X)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lstm_true, label = 'y_true')\n",
    "plt.plot(lstm_predictions, label = 'y_pred')\n",
    "plt.title('Multivariate Stacked LSTM Predictions, Area')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch out!!! The title in the output is wrong\n",
    "model_selection.loc['Multivariate Stacked LSTM, Area, Hidden Layer = 2, with regularizer'] = \\\n",
    "                    list(lstm_report(test_y, \n",
    "                         predict_lstm(13, 9, stacked_area3, data_test, scaled_X)))\n",
    "model_selection\n",
    "\n",
    "# getting worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product as Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla\n",
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset, transfer to supervised learning. The target variable is the total bookings for each week.\n",
    "product_sum = pd.read_csv('./data/product sum.csv', index_col=0, parse_dates=True).squeeze(\"columns\")\n",
    "\n",
    "test_size = 52\n",
    "data_train, data_test =  product_sum.iloc[ :-test_size,:], product_sum.iloc[-test_size:,:]\n",
    "\n",
    "# log transformation\n",
    "data_train, data_test = np.log(data_train+1), np.log(data_test+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log transformation and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset for normalization\n",
    "train_X = data_train.iloc[:,:-1]\n",
    "train_y = np.array(data_train.iloc[:,-1]).reshape(-1,1)\n",
    "test_X = data_test.iloc[:,:-1]\n",
    "test_y = np.array(data_test.iloc[:,-1]).reshape(-1,1)\n",
    "\n",
    "# normalization\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "scaler_X.fit(train_X)\n",
    "scaled_train_X = scaler_X.transform(train_X)\n",
    "scaled_test_X = scaler_X.transform(test_X)\n",
    "\n",
    "scaler_y.fit(train_y)\n",
    "scaled_train_y = scaler_y.transform(train_y)\n",
    "scaled_test_y = scaler_y.transform(test_y)\n",
    "\n",
    "scaled_X = np.append(scaled_train_X, scaled_test_X, axis=0)\n",
    "\n",
    "print(scaled_train_X.shape, scaled_test_X.shape, scaled_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_product = create_vanilla(13, 6, 512)\n",
    "vanilla_product.fit(data_generator(scaled_train_X, scaled_train_y, 13),epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Loss History of Multivariate Vanilla LSTM')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(vanilla_product.history.history['loss'], label = \"loss\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_selection = pd.DataFrame(columns=['rmse', 'mse', 'mae', 'mape'])\n",
    "\n",
    "model_selection.loc['Multivariate Vanilla LSTM, Product'] = \\\n",
    "                    list(lstm_report(test_y, \n",
    "                         predict_lstm(13, 6, vanilla_product, data_test, scaled_X)))\n",
    "model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_true = pd.Series(test_y.reshape(-1,), index = data_test.index,)\n",
    "lstm_predictions = predict_lstm(13, 6, vanilla_product, data_test, scaled_X)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lstm_true, label = 'y_true')\n",
    "plt.plot(lstm_predictions, label = 'y_pred')\n",
    "plt.title('Multivariate Vanilla LSTM Predictions, Product')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Unit 42\n",
    "### Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably the performance influenced by Unit 42. Let's delete it.\n",
    "\n",
    "# read the dataset, transfer to supervised learning. The target variable is the total bookings for each week.\n",
    "product_sum = pd.read_csv('./data/product sum.csv', index_col=0, parse_dates=True).squeeze(\"columns\")\n",
    "\n",
    "test_size = 52\n",
    "data_train, data_test =  product_sum.iloc[ :-test_size,:], product_sum.iloc[-test_size:,:]\n",
    "\n",
    "# log transformation\n",
    "data_train, data_test = np.log(data_train+1), np.log(data_test+1)\n",
    "#### Log transformation and normalization\n",
    "# split the dataset for normalization\n",
    "train_X = data_train.iloc[:,:-2]\n",
    "train_y = np.array(data_train.iloc[:,-1]).reshape(-1,1)\n",
    "test_X = data_test.iloc[:,:-2]\n",
    "test_y = np.array(data_test.iloc[:,-1]).reshape(-1,1)\n",
    "\n",
    "# normalization\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "scaler_X.fit(train_X)\n",
    "scaled_train_X = scaler_X.transform(train_X)\n",
    "scaled_test_X = scaler_X.transform(test_X)\n",
    "\n",
    "scaler_y.fit(train_y)\n",
    "scaled_train_y = scaler_y.transform(train_y)\n",
    "scaled_test_y = scaler_y.transform(test_y)\n",
    "\n",
    "scaled_X = np.append(scaled_train_X, scaled_test_X, axis=0)\n",
    "\n",
    "print(scaled_train_X.shape, scaled_test_X.shape, scaled_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_product_no42 = create_vanilla(13, 5, 512)\n",
    "vanilla_product_no42.fit(data_generator(scaled_train_X, scaled_train_y, 13),epochs=20)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Loss History of Multivariate Vanilla LSTM')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(vanilla_product_no42.history.history['loss'], label = \"loss\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# model_selection = pd.DataFrame(columns=['rmse', 'mse', 'mae', 'mape'])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lstm_true, label = 'y_true')\n",
    "plt.plot(lstm_predictions, label = 'y_pred')\n",
    "plt.title('Multivariate Vanilla LSTM Predictions, Product')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection.loc['Multivariate Vanilla LSTM, Product, No Unit 42'] = \\\n",
    "                    list(lstm_report(test_y, \n",
    "                         predict_lstm(13, 5, vanilla_product_no42, data_test, scaled_X)))\n",
    "model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked LSTM, hidden layer = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably the performance influenced by Unit 42. Let's delete it.\n",
    "\n",
    "# read the dataset, transfer to supervised learning. The target variable is the total bookings for each week.\n",
    "product_sum = pd.read_csv('./data/product sum.csv', index_col=0, parse_dates=True).squeeze(\"columns\")\n",
    "\n",
    "test_size = 52\n",
    "data_train, data_test =  product_sum.iloc[ :-test_size,:], product_sum.iloc[-test_size:,:]\n",
    "\n",
    "# log transformation\n",
    "data_train, data_test = np.log(data_train+1), np.log(data_test+1)\n",
    "#### Log transformation and normalization\n",
    "# split the dataset for normalization\n",
    "train_X = data_train.iloc[:,:-2]\n",
    "train_y = np.array(data_train.iloc[:,-1]).reshape(-1,1)\n",
    "test_X = data_test.iloc[:,:-2]\n",
    "test_y = np.array(data_test.iloc[:,-1]).reshape(-1,1)\n",
    "\n",
    "# normalization\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "scaler_X.fit(train_X)\n",
    "scaled_train_X = scaler_X.transform(train_X)\n",
    "scaled_test_X = scaler_X.transform(test_X)\n",
    "\n",
    "scaler_y.fit(train_y)\n",
    "scaled_train_y = scaler_y.transform(train_y)\n",
    "scaled_test_y = scaler_y.transform(test_y)\n",
    "\n",
    "scaled_X = np.append(scaled_train_X, scaled_test_X, axis=0)\n",
    "\n",
    "print(scaled_train_X.shape, scaled_test_X.shape, scaled_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_product_no42 = create_2_stacked(13, 5, 512, 512)\n",
    "stack_product_no42.fit(data_generator(scaled_train_X, scaled_train_y, 13),epochs=20)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Loss History of Multivariate Stacked LSTM, Product')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(stack_product_no42.history.history['loss'], label = \"loss\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# model_selection = pd.DataFrame(columns=['rmse', 'mse', 'mae', 'mape'])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lstm_true, label = 'y_true')\n",
    "plt.plot(lstm_predictions, label = 'y_pred')\n",
    "plt.title('Multivariate Stacked LSTM Predictions, Product')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection.loc['Multivariate Stacked LSTM, Product, No Unit 42, Hidden Layer = 2'] = \\\n",
    "                    list(lstm_report(test_y, \n",
    "                         predict_lstm(13, 5, stack_product_no42, data_test, scaled_X)))\n",
    "model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacled LSTM, Hidden Layer = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_product_no42_2 = create_3_stacked(13, 5, 512, 512, 512)\n",
    "stack_product_no42_2.fit(data_generator(scaled_train_X, scaled_train_y, 13),epochs=20)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Loss History of Multivariate Stacked LSTM, Product')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(stack_product_no42_2.history.history['loss'], label = \"loss\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# model_selection = pd.DataFrame(columns=['rmse', 'mse', 'mae', 'mape'])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lstm_true, label = 'y_true')\n",
    "plt.plot(lstm_predictions, label = 'y_pred')\n",
    "plt.title('Multivariate Stacked LSTM Predictions, Product')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection.loc['Multivariate Stacked LSTM, Product, No Unit 42, Hidden Layer = 3'] = \\\n",
    "                    list(lstm_report(test_y, \n",
    "                         predict_lstm(13, 5, stack_product_no42_2, data_test, scaled_X)))\n",
    "model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked LSTM with Regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# might be overfitting. Let's add regularizers to stacked_area\n",
    "\n",
    "def create_3_stacked2(n_input, n_features, neuron1, neuron2, neuron3, dropout_rate):\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(neuron1, activation='relu', input_shape = (n_input, n_features), return_sequences=True))\n",
    "    lstm_model.add(LSTM(neuron2, return_sequences=True))\n",
    "    lstm_model.add(LSTM(neuron3, recurrent_dropout = dropout_rate, recurrent_regularizer=L1L2(l1=0.001, l2=0.001)))\n",
    "    lstm_model.add(Dense(1))\n",
    "    lstm_model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError(), metrics=['mse'])\n",
    "    return lstm_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_product4 = create_3_stacked2(13, 5, 512, 512, 512, 0.1)\n",
    "stacked_product4.fit(data_generator(scaled_train_X, scaled_train_y, 13),epochs=20)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Loss History of Multivariate Stacked LSTM')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(stacked_product4.history.history['loss'], label = \"loss\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_true = pd.Series(test_y.reshape(-1,), index = data_test.index,)\n",
    "lstm_predictions = predict_lstm(13, 5, stacked_product4, data_test, scaled_X)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lstm_true, label = 'y_true')\n",
    "plt.plot(lstm_predictions, label = 'y_pred')\n",
    "plt.title('Multivariate Stacked LSTM Predictions, Area')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection.loc['Multivariate Stacked LSTM, Product, Hidden Layer = 3, with regularizer'] = \\\n",
    "                    list(lstm_report(test_y, \n",
    "                         predict_lstm(13, 5, stacked_area4, data_test, scaled_X)))\n",
    "model_selection\n",
    "\n",
    "# getting worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area + Product as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset, transfer to supervised learning. The target variable is the total bookings for each week.\n",
    "area_sum = pd.read_csv('./data/area sum.csv', index_col=0, parse_dates=True).squeeze(\"columns\")\n",
    "product_sum = pd.read_csv('./data/product sum.csv', index_col=0, parse_dates=True).squeeze(\"columns\")\n",
    "all_sum = pd.concat([pd.concat([area_sum.iloc[:,:-1], product_sum.iloc[:,:-2]], axis=1),\n",
    "            product_sum.iloc[:,-1]],axis = 1)\n",
    "\n",
    "test_size = 52\n",
    "data_train, data_test =  all_sum.iloc[ :-test_size,:], all_sum.iloc[-test_size:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transformation\n",
    "data_train, data_test = np.log(data_train+1), np.log(data_test+1)\n",
    "# split the dataset for normalization\n",
    "train_X = data_train.iloc[:,:-1]\n",
    "train_y = np.array(data_train.iloc[:,-1]).reshape(-1,1)\n",
    "test_X = data_test.iloc[:,:-1]\n",
    "test_y = np.array(data_test.iloc[:,-1]).reshape(-1,1)\n",
    "\n",
    "# normalization\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "scaler_X.fit(train_X)\n",
    "scaled_train_X = scaler_X.transform(train_X)\n",
    "scaled_test_X = scaler_X.transform(test_X)\n",
    "\n",
    "scaler_y.fit(train_y)\n",
    "scaled_train_y = scaler_y.transform(train_y)\n",
    "scaled_test_y = scaler_y.transform(test_y)\n",
    "\n",
    "scaled_X = np.append(scaled_train_X, scaled_test_X, axis=0)\n",
    "\n",
    "print(scaled_train_X.shape, scaled_test_X.shape, scaled_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layer = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_all = create_vanilla(13, 14, 512)\n",
    "vanilla_all.fit(data_generator(scaled_train_X, scaled_train_y, 13),epochs=20)\n",
    "plt.figure()\n",
    "plt.title('Loss History of Multivariate Vanilla LSTM')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(vanilla_all.history.history['loss'], label = \"loss\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_true = pd.Series(test_y.reshape(-1,), index = data_test.index,)\n",
    "lstm_predictions = predict_lstm(13, 14, vanilla_all, data_test, scaled_X)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lstm_true, label = 'y_true')\n",
    "plt.plot(lstm_predictions, label = 'y_pred')\n",
    "plt.title('Multivariate Vanilla LSTM Predictions, Hybrid')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection.loc['Multivariate Vanilla LSTM, Product + Area'] = \\\n",
    "                    list(lstm_report(test_y, \n",
    "                         predict_lstm(13, 14, vanilla_all, data_test, scaled_X)))\n",
    "model_selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layer = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_all2 = create_2_stacked(13, 14, 512, 512)\n",
    "stacked_all2.fit(data_generator(scaled_train_X, scaled_train_y, 13),epochs=20)\n",
    "plt.figure()\n",
    "plt.title('Loss History of Multivariate LSTM, hidden layer = 2')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(stacked_all2.history.history['loss'], label = \"loss\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_true = pd.Series(test_y.reshape(-1,), index = data_test.index,)\n",
    "lstm_predictions = predict_lstm(13, 14, stacked_all2, data_test, scaled_X)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lstm_true, label = 'y_true')\n",
    "plt.plot(lstm_predictions, label = 'y_pred')\n",
    "plt.title('Multivariate LSTM Predictions, Hybrid, Hidden layer = 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_report(test_y, predict_lstm(13, 14, stacked_all2, data_test, scaled_X))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layer = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_all3 = create_3_stacked(13, 14, 512, 512, 512)\n",
    "stacked_all3.fit(data_generator(scaled_train_X, scaled_train_y, 13),epochs=20)\n",
    "plt.figure()\n",
    "plt.title('Loss History of Multivariate LSTM, hidden layer = 2')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(stacked_all3.history.history['loss'], label = \"loss\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_true = pd.Series(test_y.reshape(-1,), index = data_test.index,)\n",
    "lstm_predictions = predict_lstm(13, 14, stacked_all3, data_test, scaled_X)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lstm_true, label = 'y_true')\n",
    "plt.plot(lstm_predictions, label = 'y_pred')\n",
    "plt.title('Multivariate LSTM Predictions, Hybrid, Hidden layer = 3')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_report(test_y, predict_lstm(13, 14, stacked_all3, data_test, scaled_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirenctional LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hidden layer = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirectional(n_input, n_feature, neuron):\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(Bidirectional(LSTM(neuron, activation='relu', input_shape=(n_input, n_feature))))\n",
    "    lstm_model.add(Dense(1))\n",
    "    lstm_model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError(), metrics=['mse'])\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_area = create_bidirectional(13, 9, 512)\n",
    "bidirectional_area.fit(data_generator(scaled_train_X, scaled_train_y, 13),epochs=20)\n",
    "plt.figure()\n",
    "plt.title('Loss History of Multivariate Bidirectional LSTM')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(bidirectional_area.history.history['loss'], label = \"loss\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_true = pd.Series(test_y.reshape(-1,), index = data_test.index,)\n",
    "lstm_predictions = predict_lstm(13, 9, bidirectional_area, data_test, scaled_X)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lstm_true, label = 'y_true')\n",
    "plt.plot(lstm_predictions, label = 'y_pred')\n",
    "plt.title('Multivariate Bidirectional LSTM Predictions, Area')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(lstm_report(test_y,predict_lstm(13, 9, bidirectional_area, data_test, scaled_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_area.save('./model/bidirectional_area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection.loc['Multivariate Vanilla LSTM, Product + Area'] = \\\n",
    "                    list(lstm_report(test_y, \n",
    "                         predict_lstm(13, 9, bidirectional_area, data_test, scaled_X)))\n",
    "model_selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layer = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stacked_bidirectional2(n_input, n_feature, neuron):\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(Bidirectional(LSTM(neuron, activation='relu', input_shape=(n_input, n_feature), return_sequences=True)))\n",
    "    lstm_model.add(Bidirectional(LSTM(neuron)))\n",
    "    lstm_model.add(Dense(1))\n",
    "    lstm_model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError(), metrics=['mse'])\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_area2 = create_stacked_bidirectional2(13, 9, 512)\n",
    "bidirectional_area2.fit(data_generator(scaled_train_X, scaled_train_y, 13),epochs=20)\n",
    "plt.figure()\n",
    "plt.title('Loss History of Multivariate Bidirectional LSTM, 2')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(bidirectional_area2.history.history['loss'], label = \"loss\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_true = pd.Series(test_y.reshape(-1,), index = data_test.index,)\n",
    "lstm_predictions = predict_lstm(13, 9, bidirectional_area2, data_test, scaled_X)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lstm_true, label = 'y_true')\n",
    "plt.plot(lstm_predictions, label = 'y_pred')\n",
    "plt.title('Multivariate BI-LSTM Predictions, Area, Hidden layer = 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(lstm_report(test_y,predict_lstm(13, 9, bidirectional_area2, data_test, scaled_X)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layer = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirectional3(n_input, n_feature, neuron):\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(Bidirectional(LSTM(neuron, activation='relu', input_shape=(n_input, n_feature), return_sequences=True)))\n",
    "    lstm_model.add(Bidirectional(LSTM(neuron, return_sequences=True)))\n",
    "    lstm_model.add(Bidirectional(LSTM(neuron)))\n",
    "    lstm_model.add(Dense(1))\n",
    "    lstm_model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError(), metrics=['mse'])\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_area3 = create_bidirectional3(13, 9, 512)\n",
    "bidirectional_area3.fit(data_generator(scaled_train_X, scaled_train_y, 13),epochs=20)\n",
    "plt.figure()\n",
    "plt.title('Loss History of Multivariate Bidirectional LSTM')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(bidirectional_area3.history.history['loss'], label = \"loss\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_true = pd.Series(test_y.reshape(-1,), index = data_test.index,)\n",
    "lstm_predictions = predict_lstm(13, 9, bidirectional_area3, data_test, scaled_X)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lstm_true, label = 'y_true')\n",
    "plt.plot(lstm_predictions, label = 'y_pred')\n",
    "plt.title('Multivariate BI-LSTM Predictions, Area, Hidden layer = 3')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(lstm_report(test_y,predict_lstm(13, 9, bidirectional_area3, data_test, scaled_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_predictions =  predict_lstm(13, 9, bidirectional_area, data_test, scaled_X)\n",
    "\n",
    "exp_mse = np.sqrt(mean_squared_error(np.exp(lstm_predictions),np.exp(test_y)))\n",
    "exp_mae = np.sqrt(mean_absolute_error(np.exp(lstm_predictions),np.exp(test_y)))\n",
    "print(f'{exp_mse}, {exp_mae}')\n",
    "\n",
    "compare = np.concatenate(((np.exp(lstm_predictions.values)),np.exp(test_y).reshape(1,-1)[0])).reshape(2,-1)\n",
    "compare = pd.DataFrame(compare).transpose()\n",
    "compare = compare.rename(columns={0:'prediction', 1:'bookings'})\n",
    "compare['residual'] = compare['prediction']/compare['bookings']\n",
    "plt.figure()\n",
    "plt.hist(compare['residual'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_rate = compare[(compare['residual'] >=0.80) & (compare['residual'] <=1.20)].shape[0]/compare.shape[0]\n",
    "agg_residual_rate = np.mean(compare['residual'].values)\n",
    "\n",
    "print(f'the residual that is lower than 20% is {residual_rate}, the average residual is {agg_residual_rate}..')\n",
    "compare[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_area.save('./model/vanilla_area.h5')\n",
    "vanilla_product.save('./model/vanilla_product.h5')\n",
    "vanilla_product_no42.save('./model/vanilla_product_no42.h5')\n",
    "stacked_area.save('./model/stacked_area.h5')\n",
    "stack_product_no42.save('./model/stack_product_no42.h5')\n",
    "stack_product_no42_2.save('./model/stacked_product_no42_2.h5')\n",
    "stacked_prodcut4.save('./model/stacked_prodcut.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The results shows that the product is not informative as areas. And the stacked model is not as informative as the vanilla LSTM. We will do a random search on vanilla on the area features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c690e20ef665247e3fc41fa4f53044a82e953ab60a82c44b04049d6d4ffcc0db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
